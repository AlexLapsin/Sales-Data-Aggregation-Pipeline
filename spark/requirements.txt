# PySpark ETL Job Requirements

# Core Spark and Python
pyspark==3.5.0
py4j==0.10.9.7

# Snowflake connectivity
snowflake-connector-python==3.6.0
snowflake-sqlalchemy==1.5.1

# AWS S3 integration
boto3==1.34.0
botocore==1.34.0
s3fs==2023.12.2

# Data processing and utilities
pandas==2.1.4
numpy==1.24.4
pyarrow==14.0.2

# Configuration and logging
python-dotenv==1.0.0
structlog==23.2.0

# Development and testing
pytest==7.4.3
pytest-spark==0.6.0
moto==4.2.14  # AWS mocking for tests

# Optional: Databricks utilities (if running on Databricks)
# databricks-connect==13.3.2

# Optional: Delta Lake support
# delta-spark==2.4.0
