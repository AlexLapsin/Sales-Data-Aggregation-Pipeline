# models/staging/_sources.yml
# Source table definitions for raw data in Snowflake

version: 2

sources:
  - name: silver_delta_lake
    description: "Unified batch and streaming sales data from Delta Lake Silver layer via Snowflake Delta Direct (zero-copy architecture)"
    database: SALES_DW
    schema: RAW

    meta:
      owner: "data-engineering-team"
      contains_pii: true
      technology: "Delta Direct Iceberg Table"
      data_source: "S3 Delta Lake Silver Layer"
      refresh_mode: "AUTO_REFRESH"

    freshness:
      warn_after: {count: 2, period: hour}
      error_after: {count: 6, period: hour}

    tables:
      - name: sales_silver_external
        description: "Unified sales data from both batch (CSV) and streaming (Kafka) sources, processed through Medallion architecture Bronze to Silver layers. This Iceberg table provides zero-copy access to Delta Lake tables stored in S3."

        meta:
          medallion_layer: "Silver"
          storage_format: "Delta Lake"
          access_method: "Snowflake Delta Direct"
          auto_refresh: true

        columns:
          - name: row_id
            description: "Deterministic row identifier generated from source keys to remain stable across batch and streaming loads"
            tests:
              - not_null
              - unique
            meta:
              business_key: true
          - name: source_row_id
            description: "Original upstream row identifier from Bronze (retained for lineage); may be null when unavailable"

          - name: order_id
            description: "Unique order identifier"
            tests:
              - not_null

          - name: order_date
            description: "Date when order was placed"
            tests:
              - not_null

          - name: ship_date
            description: "Date when order was shipped"

          - name: ship_mode
            description: "Shipping mode (Standard, Express, etc.)"
            tests:
              - accepted_values:
                  values: ['Standard Class', 'Second Class', 'First Class', 'Same Day']
                  config:
                    severity: warn

          - name: customer_id
            description: "Unique customer identifier"
            tests:
              - not_null

          - name: customer_name
            description: "Customer full name"

          - name: segment
            description: "Customer segment classification (Title Case with spaces per Kaggle Superstore)"
            tests:
              - accepted_values:
                  values: ['Consumer', 'Corporate', 'Home Office']

          - name: city
            description: "Customer city"

          - name: state
            description: "Customer state or province"

          - name: country
            description: "Customer country"

          - name: postal_code
            description: "Customer postal code"

          - name: market
            description: "Market region (Africa, APAC, Europe, LATAM, US, etc.)"

          - name: region
            description: "Geographic region within market"

          - name: product_id
            description: "Unique product identifier"
            tests:
              - not_null

          - name: category
            description: "Product category (Note: Snowflake Iceberg table returns uppercase despite Delta Lake source being Title Case)"
            tests:
              - accepted_values:
                  values: ['FURNITURE', 'OFFICE SUPPLIES', 'TECHNOLOGY', 'SPORTS', 'HOME', 'BOOKS', 'CLOTHING', 'ELECTRONICS']

          - name: sub_category
            description: "Product sub-category"

          - name: product_name
            description: "Product name or description"

          - name: sales
            description: "Sales amount (revenue)"
            tests:
              - not_null
              - dbt_utils.accepted_range:
                  min_value: "{{ var('min_sales') }}"
                  max_value: "{{ var('max_sales') }}"

          - name: quantity
            description: "Quantity of items sold"
            tests:
              - not_null
              - dbt_utils.accepted_range:
                  min_value: 1
                  max_value: "{{ var('max_quantity') }}"

          - name: discount
            description: "Discount percentage applied (0.0 to 1.0)"
            tests:
              - dbt_utils.accepted_range:
                  min_value: 0.0
                  max_value: "{{ var('max_discount') }}"

          - name: profit
            description: "Profit amount (can be negative)"

          - name: shipping_cost
            description: "Shipping cost for the order"
            tests:
              - dbt_utils.accepted_range:
                  min_value: 0.0
                  max_value: "{{ var('max_shipping_cost') }}"
                  config:
                    severity: warn

          - name: order_priority
            description: "Order priority level"
            tests:
              - accepted_values:
                  values: ['Low', 'Medium', 'High', 'Critical']
                  config:
                    severity: warn

          - name: source_system
            description: "Source system identifier (BATCH or KAFKA)"
            tests:
              - not_null
              - accepted_values:
                  values: ['BATCH', 'KAFKA']

          - name: processing_timestamp
            description: "Timestamp when record was processed into Silver layer"
            tests:
              - not_null

        tests:
          - dbt_utils.expression_is_true:
              expression: "ship_date >= order_date or ship_date is null"
              config:
                severity: error

          - dbt_utils.expression_is_true:
              expression: "quantity > 0"
              config:
                severity: error

          - dbt_utils.expression_is_true:
              expression: "discount >= 0 and discount <= 1"
              config:
                severity: error

        freshness:
          warn_after: {count: 1, period: hour}
          error_after: {count: 3, period: hour}
          filter: processing_timestamp >= current_timestamp() - interval '24 hours'
