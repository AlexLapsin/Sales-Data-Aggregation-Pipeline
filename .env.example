# =============================
# AWS CREDENTIALS (runtime)
# =============================
AWS_ACCESS_KEY_ID="YOUR_AWS_ACCESS_KEY_ID"
AWS_SECRET_ACCESS_KEY="YOUR_AWS_SECRET_ACCESS_KEY"
AWS_DEFAULT_REGION="YOUR_REGION"

# =============================
# DATA LAKE BUCKETS (runtime + infra)
# =============================
# Raw (bronze) bucket
S3_BUCKET="RAW_BUCKET_NAME"
# processed (silver) bucket for Parquet outputs
PROCESSED_BUCKET="PROCESSED_BUCKET_NAME"
# prefix under raw bucket
S3_PREFIX=""

# =============================
# LEGACY DATABASE CONFIGURATION - DEPRECATED ⚠️
# =============================
# These PostgreSQL/RDS variables are deprecated and moved to legacy.
# For new deployments, use Snowflake or Databricks configuration instead.
# For legacy support, see legacy/README.md
#
# ⚠️  DEPRECATION TIMELINE:
# - Q1 2025: Moved to legacy (CURRENT)
# - Q3 2025: Will be removed completely
#
# Fill this *after* legacy terraform apply using 'terraform output' db endpoint
RDS_HOST=""  # DEPRECATED: Use SNOWFLAKE_* variables instead
RDS_PORT="5432"  # DEPRECATED: Use SNOWFLAKE_* variables instead
RDS_DB="sales"  # DEPRECATED: Use SNOWFLAKE_DATABASE instead
RDS_USER="salesuser"  # DEPRECATED: Use SNOWFLAKE_USER instead
RDS_PASS="CHANGE_ME_STRONG_PASSWORD"  # DEPRECATED: Use SNOWFLAKE_PASSWORD instead

# =============================
# SNOWFLAKE CONFIGURATION (runtime) - RECOMMENDED ✅
# =============================
SNOWFLAKE_ACCOUNT="YOUR_ACCOUNT.us-east-1.aws"
SNOWFLAKE_USER="YOUR_USERNAME"
SNOWFLAKE_PASSWORD="YOUR_SNOWFLAKE_PASSWORD"
SNOWFLAKE_ROLE="SYSADMIN"
SNOWFLAKE_WAREHOUSE="COMPUTE_WH"
SNOWFLAKE_DATABASE="SALES_DW"
SNOWFLAKE_SCHEMA="RAW"
SNOWFLAKE_REGION="us-east-1"

# =============================
# DATABRICKS CONFIGURATION (runtime) - RECOMMENDED ✅
# =============================
DATABRICKS_HOST="https://YOUR_WORKSPACE.cloud.databricks.com"
DATABRICKS_TOKEN="YOUR_DATABRICKS_TOKEN"
DATABRICKS_CLUSTER_ID="YOUR_CLUSTER_ID"

# =============================
# KAFKA CONFIGURATION (runtime) - RECOMMENDED ✅
# =============================
KAFKA_BOOTSTRAP_SERVERS="localhost:9092"
KAFKA_TOPIC="sales_events"
# For producer configuration
PRODUCER_INTERVAL="1.0"
PRODUCER_BATCH_SIZE="16384"
PRODUCER_COMPRESSION="gzip"

# =============================
# TERRAFORM INPUTS (infra)
# (These will be exported as TF_VAR_* by the script.)
# Adjust names if your variables.tf differs.
# =============================
PROJECT_NAME="sales-data-pipeline"
ENVIRONMENT="dev"
ALLOWED_CIDR="YOUR.PUBLIC.IP.ADDR/32"  # e.g., 203.0.113.42/32

# LEGACY DATABASE VARIABLES - DEPRECATED ⚠️
# These are only needed if using legacy PostgreSQL components
DB_INSTANCE_CLASS="db.t3.micro"  # DEPRECATED: For legacy use only
DB_ALLOCATED_STORAGE="20"  # DEPRECATED: For legacy use only
DB_ENGINE_VERSION="15"  # DEPRECATED: For legacy use only
DB_BACKUP_RETENTION_DAYS="7"  # DEPRECATED: For legacy use only
PUBLICLY_ACCESSIBLE="true"  # DEPRECATED: For legacy use only

TRUSTED_PRINCIPAL_ARN="YOUR_ARN"

# MSK/Kafka Infrastructure Configuration - RECOMMENDED ✅
ENABLE_MSK="true"  # Enable for modern pipeline
KAFKA_VERSION="2.8.1"
BROKER_INSTANCE_TYPE="kafka.t3.small"
NUMBER_OF_BROKERS="3"

# Snowflake Infrastructure Configuration - RECOMMENDED ✅
ENABLE_SNOWFLAKE_OBJECTS="true"  # Enable for modern pipeline

# Host path for repo root (important for bind-mounts on Windows)
HOST_REPO_ROOT="FULL_PSTH_TO_YOU_PROJECT"
HOST_DATA_DIR="FULL_PSTH_TO_YOU_DATA_FOLDER"
PIPELINE_IMAGE=DOCKER_IMAGE_NAME    #e.g. sales-pipeline:latest

SALES_THRESHOLD=10000

# Optional: override Airflow admin defaults
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin
_AIRFLOW_WWW_USER_EMAIL=admin@example.com

# Dag args: override them with your own
OWNER_NAME=""
ALERT_EMAIL=""

# =============================
# RUNTIME ALIASES (for compatibility)
# Some code may expect RAW_BUCKET; map it to S3_BUCKET to avoid edits.
# =============================
RAW_BUCKET="${S3_BUCKET}"
