# =============================
# AWS CREDENTIALS (runtime)
# =============================
AWS_ACCESS_KEY_ID="YOUR_AWS_ACCESS_KEY_ID"
AWS_SECRET_ACCESS_KEY="YOUR_AWS_SECRET_ACCESS_KEY"
AWS_DEFAULT_REGION="YOUR_REGION"

# =============================
# DATA LAKE BUCKETS (runtime + infra)
# =============================
# Raw (bronze) bucket â€” must be globally unique
S3_BUCKET="sales-data-pipeline-raw"
# Optional processed (silver) bucket for Parquet outputs
PROCESSED_BUCKET="sales-data-pipeline-processed"
# Optional prefix under raw bucket (leave "" if unused)
S3_PREFIX=""

# =============================
# DATABASE (runtime)
# =============================
# Fill this *after* terraform apply using 'terraform output' db endpoint
RDS_HOST=""
RDS_PORT="5432"
RDS_DB="sales"
RDS_USER="salesuser"
RDS_PASS="CHANGE_ME_STRONG_PASSWORD"

# =============================
# TERRAFORM INPUTS (infra)
# (These will be exported as TF_VAR_* by the script.)
# Adjust names if your variables.tf differs.
# =============================
PROJECT_NAME="sales-data-pipeline"
ENVIRONMENT="dev"
ALLOWED_CIDR="YOUR.PUBLIC.IP.ADDR/32"  # e.g., 203.0.113.42/32
DB_INSTANCE_CLASS="db.t3.micro"
DB_ALLOCATED_STORAGE="20"
DB_ENGINE_VERSION="15"
DB_BACKUP_RETENTION_DAYS="7"
PUBLICLY_ACCESSIBLE="true"
TRUSTED_PRINCIPAL_ARN="YOUR_ARN"

# =============================
# RUNTIME ALIASES (for compatibility)
# Some code may expect RAW_BUCKET; map it to S3_BUCKET to avoid edits.
# =============================
RAW_BUCKET="${S3_BUCKET}"
