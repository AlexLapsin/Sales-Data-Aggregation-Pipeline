# =============================
# AWS CREDENTIALS (runtime)
# =============================
AWS_ACCESS_KEY_ID="YOUR_AWS_ACCESS_KEY_ID"
AWS_SECRET_ACCESS_KEY="YOUR_AWS_SECRET_ACCESS_KEY"
AWS_DEFAULT_REGION="YOUR_REGION"

# =============================
# DATA LAKE BUCKETS (runtime + infra)
# =============================
# Raw (bronze) bucket
S3_BUCKET="RAW_BUCKET_NAME"
# processed (silver) bucket for Parquet outputs
PROCESSED_BUCKET="PROCESSED_BUCKET_NAME"
# prefix under raw bucket
S3_PREFIX=""

# =============================
# DATABASE (runtime)
# =============================
# Fill this *after* terraform apply using 'terraform output' db endpoint
RDS_HOST=""
RDS_PORT="5432"
RDS_DB="sales"
RDS_USER="salesuser"
RDS_PASS="CHANGE_ME_STRONG_PASSWORD"

# =============================
# TERRAFORM INPUTS (infra)
# (These will be exported as TF_VAR_* by the script.)
# Adjust names if your variables.tf differs.
# =============================
PROJECT_NAME="sales-data-pipeline"
ENVIRONMENT="dev"
ALLOWED_CIDR="YOUR.PUBLIC.IP.ADDR/32"  # e.g., 203.0.113.42/32
DB_INSTANCE_CLASS="db.t3.micro"
DB_ALLOCATED_STORAGE="20"
DB_ENGINE_VERSION="15"
DB_BACKUP_RETENTION_DAYS="7"
PUBLICLY_ACCESSIBLE="true"
TRUSTED_PRINCIPAL_ARN="YOUR_ARN"

# Host path for repo root (important for bind-mounts on Windows)
HOST_REPO_ROOT="FULL_PSTH_TO_YOU_PROJECT"
HOST_DATA_DIR="FULL_PSTH_TO_YOU_DATA_FOLDER"
PIPELINE_IMAGE=DOCKER_IMAGE_NAME    #e.g. sales-pipeline:latest

SALES_THRESHOLD=10000

# Optional: override Airflow admin defaults
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin
_AIRFLOW_WWW_USER_EMAIL=admin@example.com

# =============================
# RUNTIME ALIASES (for compatibility)
# Some code may expect RAW_BUCKET; map it to S3_BUCKET to avoid edits.
# =============================
RAW_BUCKET="${S3_BUCKET}"
