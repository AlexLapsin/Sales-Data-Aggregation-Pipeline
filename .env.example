# =============================
# AWS CREDENTIALS (runtime)
# =============================
AWS_ACCESS_KEY_ID="YOUR_AWS_ACCESS_KEY_ID"
AWS_SECRET_ACCESS_KEY="YOUR_AWS_SECRET_ACCESS_KEY"
AWS_DEFAULT_REGION="YOUR_REGION"

# =============================
# DATA LAKE BUCKETS (runtime + infra)
# =============================
# Bronze layer - raw data audit trail
RAW_BUCKET="raw-sales-pipeline-ACCOUNT_ID"
# Silver layer - Delta Lake processed data
PROCESSED_BUCKET="processed-sales-pipeline-ACCOUNT_ID"
# prefix under raw bucket
S3_PREFIX=""
TRUSTED_PRINCIPAL_ARN="arn:aws:iam::ACCOUNT_ID:root"

# =============================
# SNOWFLAKE CONFIGURATION (runtime)
# =============================
SNOWFLAKE_ACCOUNT_NAME="YOUR_ACCOUNT_NAME"
SNOWFLAKE_ORGANIZATION_NAME="YOUR_ORG_NAME"
SNOWFLAKE_USER="YOUR_USERNAME"
SNOWFLAKE_PASSWORD="YOUR_SNOWFLAKE_PASSWORD"
# Use TERRAFORM_ROLE for infrastructure automation (see docs/how-to/deploy-infrastructure.md)
SNOWFLAKE_ROLE="TERRAFORM_ROLE"
SNOWFLAKE_WAREHOUSE="COMPUTE_WH"
SNOWFLAKE_DATABASE="SALES_DW"
SNOWFLAKE_SCHEMA="RAW"
ENABLE_SNOWFLAKE_OBJECTS="true"
# Key-pair authentication (recommended over password)
# Use ABSOLUTE path for local Terraform execution (not relative paths)
# Example Windows: F:/path/to/project/config/keys/rsa_key.p8
# Example Linux/Mac: /home/user/project/config/keys/rsa_key.p8
# Docker containers override this with /config/keys/rsa_key.p8 from docker-compose.yml
SNOWFLAKE_PRIVATE_KEY_PATH="/absolute/path/to/your/rsa_key.p8"
SNOWFLAKE_KEY_PASSPHRASE="YOUR_KEY_PASSPHRASE"

# =============================
# DATABRICKS CONFIGURATION (runtime)
# =============================
DATABRICKS_HOST="https://YOUR_WORKSPACE.cloud.databricks.com"
DATABRICKS_TOKEN="YOUR_DATABRICKS_TOKEN"
DATABRICKS_CLUSTER_ID="YOUR_CLUSTER_ID"

# =============================
# KAFKA CONFIGURATION (runtime)
# =============================
KAFKA_BOOTSTRAP_SERVERS="localhost:9092"
KAFKA_TOPIC="sales_events"
# For producer configuration
PRODUCER_INTERVAL="1.0"
PRODUCER_BATCH_SIZE="16384"
PRODUCER_COMPRESSION="gzip"

# =============================
# TERRAFORM INPUTS
# (These will be exported as TF_VAR_* by the script.)
# Adjust names if your variables.tf differs.
# =============================
PROJECT_NAME="sales-data-pipeline"
ENVIRONMENT="dev"
ALLOWED_CIDR="YOUR.PUBLIC.IP.ADDR/32"

TRUSTED_PRINCIPAL_ARN="YOUR_ARN"

# MSK/Kafka Infrastructure Configuration
ENABLE_MSK="false"  # Enable for modern pipeline
KAFKA_VERSION="2.8.1"
BROKER_INSTANCE_TYPE="kafka.t3.small"
NUMBER_OF_BROKERS="3"

# Host path for repo root (important for bind-mounts on Windows)
HOST_REPO_ROOT="FULL_PATH_TO_YOUR_PROJECT"
HOST_DATA_DIR="FULL_PATH_TO_YOUR_DATA_FOLDER"
PIPELINE_IMAGE=DOCKER_IMAGE_NAME    #e.g. sales-pipeline:latest

SALES_THRESHOLD=10000

# Airflow web UI credentials (change for production)
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin
_AIRFLOW_WWW_USER_EMAIL=admin@example.com

# Dag args: override them with your own
OWNER_NAME=""
ALERT_EMAIL=""
