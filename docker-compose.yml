services:

  etl-pipeline:
    build: .
    image: sales-pipeline:latest
    command: tail -f /dev/null
    env_file:
      - .env
    environment:
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      - AWS_DEFAULT_REGION
      - RDS_HOST
      - RDS_PORT
      - RDS_DB
      - RDS_USER
      - RDS_PASS
    volumes:
      - ./:/app
      - ./data:/app/data
      - ./output:/app/output
      - ./db:/app/db
      - ~/.aws:/root/.aws:ro

  airflow:
    build:
      context: .
      dockerfile: Dockerfile-airflow
    image: custom-airflow:2.6.1
    restart: always
    depends_on:
      - etl-pipeline
    ports:
      - "8080:8080"
    env_file:
      - .env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://${RDS_USER}:${RDS_PASS}@${RDS_HOST}:${RDS_PORT}/${RDS_DB}?sslmode=require"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
      - ~/.aws:/root/.aws:ro
    command:
      - bash
      - -c
      - |
        # Initialize and migrate metadata DB
        airflow db init

        # Start scheduler in background
        airflow scheduler &

        # Launch webserver as PID 1
        exec airflow webserver --port 8080

networks:
  default:
    driver: bridge
