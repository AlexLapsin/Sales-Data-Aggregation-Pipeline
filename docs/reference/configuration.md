# Configuration Reference

All environment variables and configuration settings for the pipeline.

## Environment File (.env)

All configuration is centralized in the `.env` file at project root.

### AWS Configuration

**AWS_ACCESS_KEY_ID** (required)
- Description: AWS access key for S3 and IAM operations
- Format: 20-character alphanumeric string
- Example: `AKIAIOSFODNN7EXAMPLE`
- Used by: Terraform, Airflow, Spark, data uploader

**AWS_SECRET_ACCESS_KEY** (required)
- Description: AWS secret key corresponding to access key
- Format: 40-character alphanumeric string
- Example: `YOUR_SECRET_KEY_HERE`
- Used by: Terraform, Airflow, Spark, data uploader
- Security: Never commit to git

**AWS_DEFAULT_REGION** (required)
- Description: AWS region for S3 buckets and resources
- Format: Region code
- Default: `us-east-1`
- Options: `us-east-1`, `us-west-2`, `eu-west-1`, etc.
- Used by: All AWS services

### S3 Bucket Configuration

**RAW_BUCKET** (required)
- Description: S3 bucket name for Bronze layer (raw data)
- Format: Lowercase alphanumeric with hyphens
- Example: `raw-sales-pipeline-123456`
- Naming: Must be globally unique across AWS
- Used by: Data uploader, Spark ETL, Airflow DAGs

**PROCESSED_BUCKET** (required)
- Description: S3 bucket name for Silver layer (Delta Lake)
- Format: Lowercase alphanumeric with hyphens
- Example: `processed-sales-pipeline-123456`
- Naming: Must be globally unique across AWS
- Used by: Spark ETL, dbt, Snowflake integration

**S3_PREFIX** (optional)
- Description: Prefix path within buckets
- Format: Path without leading/trailing slashes
- Default: Empty string
- Example: `data/sales`
- Used by: Advanced bucket organization

**TRUSTED_PRINCIPAL_ARN** (optional)
- Description: ARN of trusted AWS principal for cross-account access
- Format: ARN string
- Example: `arn:aws:iam::YOUR_ACCOUNT_ID:root`
- Used by: IAM trust policies

### Snowflake Configuration

**SNOWFLAKE_ACCOUNT_NAME** (required)
- Description: Snowflake account identifier
- Format: Account locator
- Example: `xy12345`
- Note: Exclude organization name and region
- Used by: Terraform provider, dbt, Python connectors

**SNOWFLAKE_ORGANIZATION_NAME** (required)
- Description: Snowflake organization name
- Format: Organization identifier
- Example: `YOUR_ORG_NAME`
- Used by: Terraform provider (modern account format)

**SNOWFLAKE_USER** (required)
- Description: Snowflake username for authentication
- Format: Username string
- Example: `ANALYTICS_USER`
- Used by: All Snowflake connections

**SNOWFLAKE_PASSWORD** (optional, legacy)
- Description: Snowflake password (deprecated, use key-pair)
- Format: Password string
- Example: Not recommended
- Security: Use SNOWFLAKE_PRIVATE_KEY_PATH instead

**SNOWFLAKE_PRIVATE_KEY_PATH** (required)
- Description: Path to encrypted RSA private key file
- Format: File path
- Example: `/config/keys/rsa_key.p8`
- Security: Encrypted key, passphrase required
- Used by: Terraform, Python connectors

**SNOWFLAKE_KEY_PASSPHRASE** (required)
- Description: Passphrase to decrypt private key
- Format: String
- Example: `YOUR_PASSPHRASE_HERE`
- Security: Never commit to git
- Used by: Key decryption in connections

**SNOWFLAKE_ROLE** (required)
- Description: Snowflake role for operations
- Format: Role name
- Default: `ACCOUNTADMIN`
- Options: `ACCOUNTADMIN`, `SYSADMIN`, `SECURITYADMIN`
- Used by: All Snowflake operations

**SNOWFLAKE_WAREHOUSE** (required)
- Description: Snowflake compute warehouse name
- Format: Warehouse name
- Default: `COMPUTE_WH`
- Example: `ANALYTICS_WH`
- Used by: Query execution, dbt runs

**SNOWFLAKE_DATABASE** (required)
- Description: Target database name
- Format: Database name
- Default: `SALES_DW`
- Example: `SALES_DB`
- Used by: dbt, data loading, queries

**SNOWFLAKE_SCHEMA** (required)
- Description: Target schema name
- Format: Schema name
- Default: `RAW`
- Example: `PUBLIC`
- Used by: Table creation, dbt models

**ENABLE_SNOWFLAKE_OBJECTS** (optional)
- Description: Enable Snowflake resource creation in Terraform
- Format: Boolean string
- Default: `true`
- Options: `true`, `false`
- Used by: Terraform conditional deployment

**SNOWFLAKE_EXTERNAL_ID** (optional)
- Description: External ID for IAM trust policy
- Format: Generated by Snowflake
- Example: `ABC123_SFCRole=...`
- Used by: Storage integration trust

### Databricks Configuration

**DATABRICKS_HOST** (optional)
- Description: Databricks workspace URL
- Format: HTTPS URL
- Example: `https://your-workspace.cloud.databricks.com`
- Used by: Spark job submission
- Note: If not set, uses local Spark

**DATABRICKS_TOKEN** (optional)
- Description: Databricks personal access token
- Format: Token string starting with `dapi`
- Example: `dapi1234567890abcdef`
- Security: Never commit to git
- Used by: API authentication

**DATABRICKS_CLUSTER_ID** (optional)
- Description: Databricks cluster ID for job execution
- Format: Cluster ID string
- Example: `1234-567890-abcdef12`
- Used by: Job submission
- Note: If not set, creates job cluster

### Kafka Configuration

**KAFKA_BOOTSTRAP_SERVERS** (required)
- Description: Kafka broker addresses
- Format: Comma-separated host:port
- Default: `kafka:9092`
- Example: `localhost:9092` (external), `kafka:9092` (Docker)
- Used by: Producers, consumers, connectors

**KAFKA_TOPIC** (required)
- Description: Kafka topic name for sales events
- Format: Topic name
- Default: `sales_events`
- Used by: Producer, consumer, connector

**PRODUCER_INTERVAL** (optional)
- Description: Seconds between producer events
- Format: Float
- Default: `1.0`
- Example: `0.5` (2 events/sec), `5.0` (1 event/5sec)
- Used by: sales_event_producer.py

**PRODUCER_BATCH_SIZE** (optional)
- Description: Producer batch size in bytes
- Format: Integer
- Default: `16384` (16KB)
- Used by: Kafka producer configuration

**PRODUCER_COMPRESSION** (optional)
- Description: Compression codec for messages
- Format: String
- Default: `gzip`
- Options: `none`, `gzip`, `snappy`, `lz4`, `zstd`
- Used by: Kafka producer

### Project Settings

**PROJECT_NAME** (required)
- Description: Project name for resource naming
- Format: Lowercase alphanumeric with hyphens
- Default: `sales-data-pipeline`
- Used by: Terraform resource naming, tagging

**ENVIRONMENT** (required)
- Description: Environment name
- Format: String
- Default: `dev`
- Options: `dev`, `staging`, `prod`
- Used by: Resource tagging, naming conventions

**SALES_THRESHOLD** (optional)
- Description: Threshold for sales anomaly detection
- Format: Integer
- Default: `10000`
- Used by: Data quality checks

**ALLOWED_CIDR** (optional)
- Description: CIDR block for network access
- Format: CIDR notation
- Example: `YOUR_IP_ADDRESS/32`
- Used by: Security group rules (production)

### Airflow Settings

**_AIRFLOW_WWW_USER_USERNAME** (required)
- Description: Airflow web UI admin username
- Format: String
- Default: `admin`
- Used by: Airflow authentication

**_AIRFLOW_WWW_USER_PASSWORD** (required)
- Description: Airflow web UI admin password
- Format: String
- Default: `admin`
- Security: Change for production
- Used by: Airflow authentication

**_AIRFLOW_WWW_USER_EMAIL** (required)
- Description: Airflow admin email
- Format: Email address
- Default: `admin@example.com`
- Used by: Airflow user management

**OWNER_NAME** (optional)
- Description: Default DAG owner name
- Format: String
- Example: `data-engineering-team`
- Used by: DAG default args

**ALERT_EMAIL** (optional)
- Description: Email for DAG failure alerts
- Format: Email address
- Example: `alerts@example.com`
- Used by: Airflow email alerts

### Docker Settings

**HOST_REPO_ROOT** (required for Windows)
- Description: Full path to project repository
- Format: Absolute path
- Example: `F:/GITHUB/sales_data_aggregation_pipeline`
- Used by: Docker volume mounts on Windows

**HOST_DATA_DIR** (optional)
- Description: Full path to data directory
- Format: Absolute path
- Example: `F:/GITHUB/sales_data_aggregation_pipeline/data`
- Used by: Data volume mounts

**PIPELINE_IMAGE** (optional)
- Description: Docker image name for custom builds
- Format: Image name with tag
- Default: `sales-pipeline:latest`
- Used by: Docker Compose image references

### MSK/Kafka Infrastructure (Production)

**ENABLE_MSK** (optional)
- Description: Enable AWS MSK (Managed Kafka) deployment
- Format: Boolean string
- Default: `false`
- Options: `true`, `false`
- Used by: Terraform conditional deployment

**KAFKA_VERSION** (optional)
- Description: Apache Kafka version for MSK
- Format: Version string
- Default: `2.8.1`
- Used by: MSK cluster configuration

**BROKER_INSTANCE_TYPE** (optional)
- Description: EC2 instance type for Kafka brokers
- Format: Instance type
- Default: `kafka.t3.small`
- Options: `kafka.t3.small`, `kafka.m5.large`, etc.
- Used by: MSK broker sizing

**NUMBER_OF_BROKERS** (optional)
- Description: Number of Kafka broker nodes
- Format: Integer
- Default: `3`
- Minimum: 2 (high availability)
- Used by: MSK cluster sizing

## Terraform Variables

Terraform reads from environment variables prefixed with `TF_VAR_`.

### Exporting Variables

Use the provided script:

```bash
source export_tf_vars.sh
```

This exports all `.env` variables as `TF_VAR_*` equivalents.

### Variable Mapping

| .env Variable | Terraform Variable | Module |
|---------------|-------------------|---------|
| AWS_ACCESS_KEY_ID | TF_VAR_AWS_ACCESS_KEY_ID | providers |
| AWS_SECRET_ACCESS_KEY | TF_VAR_AWS_SECRET_ACCESS_KEY | providers |
| RAW_BUCKET | TF_VAR_RAW_BUCKET | s3 |
| PROCESSED_BUCKET | TF_VAR_PROCESSED_BUCKET | s3 |
| SNOWFLAKE_ACCOUNT_NAME | TF_VAR_SNOWFLAKE_ACCOUNT_NAME | snowflake |
| SNOWFLAKE_PRIVATE_KEY_PATH | TF_VAR_SNOWFLAKE_PRIVATE_KEY_PATH | snowflake |

## dbt Configuration

### profiles.yml

Located at `dbt/profiles.yml` (gitignored, created from template):

```yaml
sales_pipeline:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: "{{ env_var('SNOWFLAKE_ACCOUNT_NAME') }}.{{ env_var('SNOWFLAKE_ORGANIZATION_NAME') }}"
      user: "{{ env_var('SNOWFLAKE_USER') }}"
      private_key_path: "{{ env_var('SNOWFLAKE_PRIVATE_KEY_PATH') }}"
      private_key_passphrase: "{{ env_var('SNOWFLAKE_KEY_PASSPHRASE') }}"
      role: "{{ env_var('SNOWFLAKE_ROLE') }}"
      database: "{{ env_var('SNOWFLAKE_DATABASE') }}"
      warehouse: "{{ env_var('SNOWFLAKE_WAREHOUSE') }}"
      schema: "{{ env_var('SNOWFLAKE_SCHEMA') }}"
      threads: 4
```

### dbt_project.yml

Located at `dbt/dbt_project.yml`:

```yaml
name: 'sales_pipeline'
version: '1.0.0'
config-version: 2

profile: 'sales_pipeline'

model-paths: ["models"]
test-paths: ["tests"]
macro-paths: ["macros"]

models:
  sales_pipeline:
    staging:
      +materialized: view
    intermediate:
      +materialized: table
    marts:
      +materialized: table
      +cluster_by: ["order_date"]
```

## Docker Compose

### Service Environment Variables

Docker Compose inherits from `.env` automatically.

**Airflow Services:**
```yaml
environment:
  AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
  AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
  SNOWFLAKE_ACCOUNT: ${SNOWFLAKE_ACCOUNT_NAME}
  SNOWFLAKE_USER: ${SNOWFLAKE_USER}
  SNOWFLAKE_PRIVATE_KEY_PATH: /config/keys/rsa_key.p8
  SNOWFLAKE_KEY_PASSPHRASE: ${SNOWFLAKE_KEY_PASSPHRASE}
```

**Kafka Services:**
```yaml
environment:
  KAFKA_BOOTSTRAP_SERVERS: kafka:9092
  KAFKA_TOPIC: ${KAFKA_TOPIC}
```

## Validation

### Required Variables Check

Minimum required variables for pipeline operation:

```bash
# AWS
AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY
AWS_DEFAULT_REGION

# S3
RAW_BUCKET
PROCESSED_BUCKET

# Snowflake
SNOWFLAKE_ACCOUNT_NAME
SNOWFLAKE_ORGANIZATION_NAME
SNOWFLAKE_USER
SNOWFLAKE_PRIVATE_KEY_PATH
SNOWFLAKE_KEY_PASSPHRASE
SNOWFLAKE_ROLE
SNOWFLAKE_WAREHOUSE
SNOWFLAKE_DATABASE
SNOWFLAKE_SCHEMA
```

### Validate Configuration

```bash
python tools/validation/config_validator.py --validate-all
```

Checks:
- All required variables present
- AWS credentials valid
- Snowflake connection successful
- S3 buckets accessible
- Format validation

## Security Best Practices

**Never commit:**
- `.env` file
- Private keys (`*.pem`, `*.p8`)
- AWS credentials
- Snowflake credentials
- Databricks tokens

**Use gitignore:**
```gitignore
.env
.env.local
.env.prod
*.pem
*.p8
rsa_key*
```

**Production:**
- Use AWS Secrets Manager
- Rotate credentials regularly
- Use IAM roles where possible
- Enable MFA on AWS accounts
- Use Snowflake network policies

## Environment-Specific Configuration

### Development (.env)

```bash
ENVIRONMENT=dev
AWS_DEFAULT_REGION=us-east-1
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
DATABRICKS_HOST=  # Empty, uses local Spark
```

### Production (.env.prod)

```bash
ENVIRONMENT=prod
AWS_DEFAULT_REGION=us-east-1
SNOWFLAKE_WAREHOUSE=ANALYTICS_WH_LARGE
DATABRICKS_HOST=https://your-workspace.cloud.databricks.com
DATABRICKS_TOKEN=YOUR_PROD_TOKEN_HERE
```

Load production config:
```bash
cp .env.prod .env
source export_tf_vars.sh
```

## Python Requirements Structure

The project uses modular requirements files to manage dependencies across different container types.

### Requirements Files

**requirements/core.txt**
- Base dependencies for non-Airflow containers
- Used by: Spark, dbt, ETL containers
- Contains: pandas, numpy, boto3, snowflake-connector, databricks-connector
- Modern library versions (no Airflow constraints)

**requirements/spark.txt**
- PySpark-specific dependencies
- Includes: `-r core.txt`
- Adds: pyspark, delta-spark, py4j

**requirements/streaming.txt**
- Kafka streaming dependencies
- Includes: `-r core.txt`
- Adds: faker, click, prometheus-client

**requirements/cloud.txt**
- Cloud-specific packages
- Includes: `-r core.txt`
- Adds: snowflake-sqlalchemy, databricks-connect

**requirements/test.txt**
- Testing framework dependencies
- Includes: `-r core.txt`
- Adds: pytest, pytest-cov, moto, faker

**requirements/validation.txt**
- Configuration validation tools
- Includes: `-r core.txt`
- Adds: PyYAML, jsonschema, psutil

**requirements/dev.txt**
- Development tools (standalone)
- Contains: black, flake8, mypy, pre-commit
- Does not include other requirements

### Dockerfile Patterns

**Spark Container:**
```dockerfile
FROM apache/spark:3.5.7-python3
USER root
RUN apt-get update && apt-get install -y gcc
COPY requirements/spark.txt /app/requirements/spark.txt
RUN pip install --no-cache-dir -r /app/requirements/spark.txt
```

**dbt Container:**
```dockerfile
FROM python:3.10-slim
COPY requirements/core.txt /app/requirements/core.txt
RUN pip install --no-cache-dir -r /app/requirements/core.txt
RUN pip install dbt-snowflake>=1.7.0
```

**ETL Container:**
```dockerfile
FROM python:3.11-slim
COPY requirements/core.txt /app/requirements/core.txt
COPY requirements/streaming.txt /app/requirements/streaming.txt
RUN pip install --no-cache-dir -r /app/requirements/core.txt
RUN pip install --no-cache-dir -r /app/requirements/streaming.txt
```

**Airflow Container (Inline Installation):**
```dockerfile
FROM apache/airflow:2.10.0
USER airflow

# Install providers and utilities
RUN pip install --no-cache-dir \
    apache-airflow-providers-docker \
    apache-airflow-providers-snowflake \
    astronomer-cosmos>=1.3.0

# Install dbt in isolated virtual environment
RUN python -m venv dbt_venv && source dbt_venv/bin/activate && \
    pip install --no-cache-dir dbt-core>=1.7.0 dbt-snowflake>=1.7.0 && \
    deactivate
```

### Base Image Versions

**Current Docker images:**
- Apache Airflow: `apache/airflow:2.10.0` (October 2024)
- Apache Spark: `apache/spark:3.5.7-python3` (September 2024)
- Python (dbt/ETL): `python:3.10-slim` / `python:3.11-slim`
- Delta Lake: `delta-spark==3.2.0` (compatible with Spark 3.5.x)

### Design Rationale

**Why Airflow Uses Inline Installation:**

Airflow does not use a requirements file due to dependency conflicts with dbt:

1. **Technical Conflict:**
   - dbt-core requires protobuf>=5.0,<6.0
   - Airflow providers require different protobuf versions
   - pip cannot resolve these conflicts (ResolutionTooDeep error)

2. **Industry Best Practice:**
   - Astronomer (Cosmos maintainers) recommend installing dbt in an isolated virtual environment
   - Prevents "Python dependency hell" between Airflow and dbt
   - Source: https://github.com/astronomer/cosmos-use-case

3. **Virtual Environment Isolation:**
   - Airflow providers installed in main Python environment
   - dbt installed in `dbt_venv` virtual environment
   - Cosmos automatically detects and uses the dbt virtual environment

**Spark Base Image Selection:**

The Spark container uses `apache/spark:3.5.7-python3` instead of older versions for:

1. **Build Performance:**
   - Older images (3.5.0) based on Ubuntu Focal 20.04 (April 2020)
   - 5+ years of accumulated package updates cause slow apt-get operations
   - Newer image has fresher base with fewer accumulated updates

2. **Security:**
   - Latest security patches for Ubuntu base
   - Recent CVE fixes in system packages

3. **Compatibility:**
   - Spark 3.5.7 maintains compatibility with Delta Lake 3.2.0
   - Same Spark 3.5.x major version line
   - No breaking changes for existing PySpark code

**Other Containers:**
- Use requirements files normally
- No dependency conflicts
- Modular includes via `-r` directive
- Single source of truth per container type
